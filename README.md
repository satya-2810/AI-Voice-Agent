# ğŸ¤ AI Voice Chat Agent

A sophisticated conversational AI agent that supports voice input and output, built with FastAPI and modern Python practices. The agent processes speech through a complete pipeline: Speech-to-Text â†’ Language Model â†’ Text-to-Speech.

![Architecture of the pipeline](assets/architecture_diagram.png "Architecture of the pipeline") ![Voice agent ui](assets/ui_interface.png "Voice agent UI Interface")

## âœ¨ Features

- ğŸ™ï¸ **Voice Input**: Record audio directly in the browser
- ğŸ—£ï¸ **Voice Output**: High-quality text-to-speech responses
- ğŸ’¬ **Chat Memory**: Maintains conversation context across interactions
- ğŸ—ï¸ **Modern Architecture**: Clean, maintainable code with service separation
- ğŸ“Š **Comprehensive Logging**: Detailed logs for debugging and monitoring

## ğŸ› ï¸ Technology Stack

- **Backend**: FastAPI with Pydantic models
- **Speech-to-Text**: AssemblyAI
- **Language Model**: Google Gemini Pro
- **Text-to-Speech**: Murf AI
- **Frontend**: JavaScript

## ğŸ“‹ Prerequisites

- Python 3.11+
- API Keys for:
  - Google Gemini API
  - Murf AI API
  - AssemblyAI API

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/ai-voice-chat-agent.git
cd ai-voice-chat-agent
```

### 2. Set Up Environment

```bash
# Create virtual environment
python -m venv venv
source venv/Scripts/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configure Environment Variables

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys
GEMINI_API_KEY=your_gemini_api_key_here
MURF_API_KEY=your_murf_api_key_here
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
```

### 4. Run the Application

```bash

# With uvicorn
uvicorn app:app --reload
```

## ğŸ“ Project Structure

```plaintext
voice-agent/
â”œâ”€â”€ app.py                 # Main FastAPI application
â”œâ”€â”€ config.py              # Configuration management
â”œâ”€â”€ run.py                 # Development server runner
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example           # Environment variables template
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ schemas.py         # Pydantic models
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ stt_service.py     # Speech-to-Text service
â”‚   â”œâ”€â”€ llm_service.py     # Language Model service
â”‚   â”œâ”€â”€ tts_service.py     # Text-to-Speech service
â”‚   â””â”€â”€ chat_service.py    # Chat history management
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logger.py          # Logging configuration
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ script.js          # Frontend JavaScript
â”‚   â””â”€â”€ style.css          # Stylesheets
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # HTML template
â””â”€â”€ temp/                  # Temporary audio files
```

## ğŸ—ï¸ Architecture Overview

The application follows a clean architecture pattern with separation of concerns:

### Services Layer

- **STTService**: Handles AssemblyAI speech-to-text operations
- **LLMService**: Manages Gemini API interactions
- **TTSService**: Processes Murf AI text-to-speech
- **ChatService**: Maintains conversation history

## ğŸ”„ Data Flow

```mermaid
graph TD
    A[ğŸ™ï¸ Voice Input] --> B[ğŸ“¤ Upload Audio]
    B --> C[ğŸ¯ Speech-to-Text]
    C --> D[ğŸ’¾ Save to History]
    D --> E[ğŸ¤– Generate Response]
    E --> F[ğŸ’¾ Save Response]
    F --> G[ğŸ”Š Text-to-Speech]
    G --> H[ğŸ“± Return Audio + Text]
```

## ğŸ¯ Usage Examples

### Voice Interaction

1. Click "ğŸ¤ Start Recording"
2. Speak your message
3. Click "â¹ Stop Recording"
4. Receive AI response with audio playback

### Text Query (API)

```bash
curl -X POST "http://localhost:8000/llm/query" \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello, how are you today?"}'
```

### Get Session History

```bash
curl -X GET "http://localhost:8000/sessions/123456/history"
```

### Example Responses

#### Successful Chat Response

```json
{
  "text": "Hello! How can I help you today?",
  "audio_base64": "UklGRiQAAABXQVZFZm10...",
  "session_id": "1234567890"
}
```

#### Error Response

```json
{
  "error": "Transcription failed",
  "detail": "No speech detected in audio",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

## ğŸ™ Acknowledgments

- **AssemblyAI** for speech recognition services
- **Google Gemini** for language model capabilities
- **Murf AI** for text-to-speech synthesis
- **FastAPI** for the excellent web framework

## ğŸŒŸ Support

If you find this project helpful, please consider:

- â­ Giving it a star on GitHub
- ğŸ´ Forking and contributing
- ğŸ“¢ Sharing with the community
- ğŸ’¬ Opening issues for bugs or features
